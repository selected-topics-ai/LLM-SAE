{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T23:44:21.354087Z",
     "start_time": "2025-04-21T23:44:17.892475Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from sae_lens import SAE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:44:21.363910Z",
     "start_time": "2025-04-21T23:44:21.358027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(16)\n",
    "torch.set_grad_enabled(False)"
   ],
   "id": "14b786d30bd76cb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x31b496ea0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:44:27.677120Z",
     "start_time": "2025-04-21T23:44:22.283419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ],
   "id": "b44080baff145665",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad152cd21e1d4b22963d78e14332a80d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:52:53.636959Z",
     "start_time": "2025-04-21T23:52:48.694263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Who are you?\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"mps\")\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = \"layer_20/width_16k/canonical\",\n",
    "    device = \"mps\"\n",
    ")\n",
    "\n",
    "def generate_with_sae(model,\n",
    "                      sae: SAE,\n",
    "                      layer: int,\n",
    "                      feature_index: int,\n",
    "                      alpha: float,\n",
    "                      do_sample: bool,\n",
    "                      temperature: float,\n",
    "                      max_len: int,\n",
    "                      repetition_penalty:float=1.0,\n",
    "                      normalize_new_hidden=True):\n",
    "\n",
    "    def gather_act_hook(mod, inputs, outputs):\n",
    "\n",
    "        out = outputs[0]\n",
    "\n",
    "        try:\n",
    "            sparse_out = sae.encode_jumprelu(out)\n",
    "            to_add = torch.zeros_like(sparse_out)\n",
    "            to_add[:, :, feature_index] = alpha\n",
    "            new_out = sparse_out + to_add\n",
    "            out = sae.decode(new_out)\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            out = outputs[0]\n",
    "\n",
    "        return (out,)\n",
    "\n",
    "    def gather_act_hook_normalized(mod, inputs, outputs):\n",
    "\n",
    "        out = outputs[0]\n",
    "\n",
    "        try:\n",
    "            sparse_out = sae.encode_jumprelu(out)\n",
    "            to_add = torch.zeros_like(sparse_out)\n",
    "            to_add[:, :, feature_index] = alpha\n",
    "\n",
    "            new_out = sparse_out + to_add\n",
    "            new_out = (new_out / new_out.norm(dim=-1, keepdim=True)) * sparse_out.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            out = sae.decode(new_out)\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            out = outputs[0]\n",
    "\n",
    "        return (out,)\n",
    "\n",
    "    if normalize_new_hidden:\n",
    "        hook = model.model.layers[layer].register_forward_hook(gather_act_hook_normalized)\n",
    "    else:\n",
    "        hook = model.model.layers[layer].register_forward_hook(gather_act_hook)\n",
    "\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_length=max_len,\n",
    "                             temperature=temperature,\n",
    "                             do_sample=do_sample,\n",
    "                             repetition_penalty=repetition_penalty,\n",
    "                             use_cache=False)\n",
    "    hook.remove()\n",
    "\n",
    "    return outputs\n"
   ],
   "id": "29bb8074fe7957",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:54:50.982976Z",
     "start_time": "2025-04-21T23:54:47.521812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = generate_with_sae(model,\n",
    "                            sae,\n",
    "                            layer=20,\n",
    "                            feature_index=12082,\n",
    "                            alpha=40.0,\n",
    "                            do_sample=False,\n",
    "                            temperature=0,\n",
    "                            max_len=50,\n",
    "                            repetition_penalty=1.0)"
   ],
   "id": "b93c01d653643539",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:54:51.013515Z",
     "start_time": "2025-04-21T23:54:51.011042Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(outputs[0])",
   "id": "b96b7d1a038dba31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Who are you?\\n \\n I am a large-scale language model, trained by Google.\\n \\n What is your purpose?\\n \\n My purpose is to help people understand and interact with information. I can translate languages, write'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3659b4bc26673366"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
